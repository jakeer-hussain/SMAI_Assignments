{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UTKFaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [f for f in os.listdir(root_dir) if f.endswith(\".jpg\")]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_paths[idx]\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        # Extract age from filename\n",
    "        age = int(re.match(r\"(\\d+)_\", img_name).group(1))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(age, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"archive (1)/utkface_aligned_cropped/UTKFace\"\n",
    "dataset = UTKFaceDataset(data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AgeCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for images, ages in train_loader:\n",
    "            images, ages = images.to(device), ages.to(device).view(-1, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, ages)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, ages in test_loader:\n",
    "            images, ages = images.to(device), ages.to(device).view(-1, 1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, ages)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 261.0037\n",
      "Epoch 2, Train Loss: 128.8058\n",
      "Epoch 3, Train Loss: 97.1538\n",
      "Epoch 4, Train Loss: 78.1315\n",
      "Epoch 5, Train Loss: 62.5130\n",
      "Epoch 6, Train Loss: 50.5656\n",
      "Epoch 7, Train Loss: 39.4348\n",
      "Epoch 8, Train Loss: 30.0549\n",
      "Epoch 9, Train Loss: 25.2342\n",
      "Epoch 10, Train Loss: 23.0450\n",
      "Test MSE Loss: 82.0894\n"
     ]
    }
   ],
   "source": [
    "cnn_model = AgeCNN()\n",
    "cnn_mse = train_model(cnn_model, train_loader, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 159.2431\n",
      "Epoch 2, Train Loss: 75.8981\n",
      "Epoch 3, Train Loss: 66.4541\n",
      "Epoch 4, Train Loss: 54.1481\n",
      "Epoch 5, Train Loss: 43.9015\n",
      "Epoch 6, Train Loss: 36.4273\n",
      "Epoch 7, Train Loss: 28.6846\n",
      "Epoch 8, Train Loss: 23.8080\n",
      "Epoch 9, Train Loss: 21.0398\n",
      "Epoch 10, Train Loss: 16.7118\n",
      "Test MSE Loss: 56.4122\n",
      "CNN Test MSE: 82.0894, ResNet-18 Test MSE: 56.4122\n"
     ]
    }
   ],
   "source": [
    "# Train ResNet-18\n",
    "resnet18 = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the fully connected layer for regression (1 output)\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 1)\n",
    "\n",
    "resnet_mse = train_model(resnet18, train_loader, test_loader)\n",
    "\n",
    "print(f\"CNN Test MSE: {cnn_mse:.4f}, ResNet-18 Test MSE: {resnet_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- > Which model performs better and why?\n",
    "\n",
    "Based on the results:\n",
    "\n",
    "CNN Test MSE: 82.0894\n",
    "\n",
    "ResNet-18 Test MSE: 56.4122\n",
    "\n",
    "Since lower MSE (Mean Squared Error) indicates better performance, ResNet-18 performs better than the CNN trained from scratch.\n",
    "\n",
    "- > WWhy Does ResNet-18 Perform Better?\n",
    "\n",
    "1. Pretrained Weights & Transfer Learning\n",
    "\n",
    "ResNet-18 starts with pretrained weights from the ImageNet dataset, which means it has already learned to recognize important visual patterns like edges, textures, shapes, and facial structures. Since these features are universal across different types of images, ResNet-18 can quickly adapt to a new task like age prediction.\n",
    "\n",
    "On the other hand, a custom CNN trained from scratch has no prior knowledge. It starts with random weights and has to learn everything from zero, including basic patterns. This process takes longer, requires more data, and often results in worse performance compared to a model that benefits from transfer learning.\n",
    "\n",
    "2. Deeper & More Optimized Architecture\n",
    "\n",
    "ResNet-18 is a much deeper network compared to a typical CNN, meaning it has more layers that help capture detailed patterns in images. A deeper network allows for better feature extraction, identifying important details such as wrinkles, facial contours, and skin texture, which are useful for age prediction.\n",
    "\n",
    "On the other hand, a custom CNN trained from scratch likely had fewer layers and parameters, meaning it had a limited ability to recognize complex patterns. With fewer layers, the model may struggle to differentiate subtle age-related features, leading to lower accuracy compared to ResNet-18.\n",
    "\n",
    "3. Generalization Ability\n",
    "\n",
    "The pretrained ResNet-18 already has a strong feature extraction capability from large-scale datasets.\n",
    "\n",
    "The scratch CNN may have suffered from overfitting or struggled to extract meaningful high-level patterns, leading to higher test error.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "ResNet-18 is the better model because it uses transfer learning, has a deeper architecture, and generalizes better.\n",
    "\n",
    "If the dataset were much larger, the scratch CNN might have performed better after sufficient training, but with limited data, ResNet-18 is the preferred choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
