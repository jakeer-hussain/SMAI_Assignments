{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def load_embeddings(file_path):\n",
    "    return torch.load(file_path).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine_distance(text_sample, train_sample):\n",
    "    \n",
    "    dot_product = np.dot(text_sample, train_sample)  # Dot product of the vectors\n",
    "    norm_text = np.linalg.norm(text_sample)  # Norm of the text sample\n",
    "    norm_train = np.linalg.norm(train_sample)  # Norm of the train sample\n",
    "    \n",
    "    if norm_text == 0 or norm_train == 0:\n",
    "        return 1.0\n",
    "\n",
    "    cosine_similarity = dot_product / (norm_text * norm_train)\n",
    "    \n",
    "    cosine_distance = 1 - cosine_similarity\n",
    "    return cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(text_embedding, train_embeddings, k):\n",
    "    distances = []\n",
    "    \n",
    "    for idx, train_embedding in enumerate(train_embeddings):\n",
    "        dist = Cosine_distance(text_embedding, train_embedding)\n",
    "        distances.append((dist, idx))\n",
    "\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    \n",
    "    return [idx for _, idx in distances[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank_1(nearest_neighbors, train_labels):\n",
    "    mrr = 0.0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = i\n",
    "\n",
    "        neighbors = nearest_neighbors[i]\n",
    "        for rank, neighbor_idx in enumerate(neighbors):\n",
    "            if train_labels[neighbor_idx] == true_label:\n",
    "                mrr += 1 / (rank + 1)  \n",
    "                break\n",
    "        \n",
    "    return mrr / len(nearest_neighbors)\n",
    "\n",
    "def precision_at_k_1(nearest_neighbors, train_labels, k=100):\n",
    "    precision = 0.0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = i\n",
    "        neighbors = nearest_neighbors[i]\n",
    "\n",
    "        relevant_count = 0\n",
    "        for neighbor_idx in neighbors[:k]:\n",
    "            if train_labels[neighbor_idx] == true_label:\n",
    "                relevant_count += 1\n",
    "        precision += relevant_count / k\n",
    "        \n",
    "    return precision / len(nearest_neighbors)\n",
    "\n",
    "def hit_rate_at_k_1(nearest_neighbors, train_labels, k=100):\n",
    "    hits = 0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = i\n",
    "        neighbors = nearest_neighbors[i]\n",
    "\n",
    "        if any(train_labels[neighbor_idx] == true_label for neighbor_idx in neighbors[:k]):\n",
    "            hits += 1\n",
    "        \n",
    "    return hits / len(nearest_neighbors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reciprocal_rank_2(nearest_neighbors, train_labels, test_labels):\n",
    "    mrr = 0.0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = test_labels[i]\n",
    "\n",
    "        neighbors = nearest_neighbors[i]\n",
    "\n",
    "        for rank, neighbor_idx in enumerate(neighbors):\n",
    "            if train_labels[neighbor_idx] == true_label:\n",
    "                mrr += 1 / (rank + 1)\n",
    "                break\n",
    "        \n",
    "    return mrr / len(nearest_neighbors)\n",
    "\n",
    "\n",
    "def precision_at_k_2(nearest_neighbors, train_labels, test_labels, k=100):\n",
    "    precision = 0.0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = test_labels[i] \n",
    "        neighbors = nearest_neighbors[i]\n",
    "\n",
    "        relevant_count = 0\n",
    "        for neighbor_idx in neighbors[:k]:\n",
    "            if train_labels[neighbor_idx] == true_label:\n",
    "                relevant_count += 1\n",
    "        precision += relevant_count / k\n",
    "        \n",
    "    return precision / len(nearest_neighbors)\n",
    "\n",
    "def hit_rate_at_k_2(nearest_neighbors, train_labels, test_labels,k=100):\n",
    "    hits = 0\n",
    "    for i in range(len(nearest_neighbors)):\n",
    "        true_label = test_labels[i] \n",
    "        neighbors = nearest_neighbors[i]\n",
    "\n",
    "        if any(train_labels[neighbor_idx] == true_label for neighbor_idx in neighbors[:k]):\n",
    "            hits += 1\n",
    "        \n",
    "    return hits / len(nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = load_embeddings('SMAI A1-20250202T180732Z-001/SMAI A1/train_embeddings.pth')\n",
    "text_embeddings = load_embeddings('SMAI A1-20250202T180732Z-001/SMAI A1/text_embedding.pth')\n",
    "train_labels = load_embeddings('SMAI A1-20250202T180732Z-001/SMAI A1/train_labels.pth')\n",
    "test_embeddings = load_embeddings('SMAI A1-20250202T180732Z-001/SMAI A1/test_embeddings.pth')\n",
    "test_labels = load_embeddings('SMAI A1-20250202T180732Z-001/SMAI A1/test_labels.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 1.0\n",
      "Precision@100: 0.974\n",
      "Hit Rate@100: 1.0\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "nearest_neighbors = []\n",
    "\n",
    "for text in text_embeddings:\n",
    "    nearest_embeds = find_k_nearest_neighbors(text, train_embeddings, k)\n",
    "    nearest_neighbors.append(nearest_embeds)\n",
    "\n",
    "mrr = mean_reciprocal_rank_1(nearest_neighbors, train_labels)\n",
    "print(f\"MRR: {mrr}\")\n",
    "precision = precision_at_k_1(nearest_neighbors, train_labels, k=100)\n",
    "hit_rate = hit_rate_at_k_1(nearest_neighbors, train_labels, k=100)\n",
    "\n",
    "print(f\"Precision@100: {precision}\")\n",
    "print(f\"Hit Rate@100: {hit_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.9347961513315047\n",
      "Precision@100: 0.8410819999999664\n",
      "Hit Rate@100: 0.9996\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "nearest_neighbors = []\n",
    "\n",
    "for test_point in test_embeddings:\n",
    "    nearest_embeds = find_k_nearest_neighbors(test_point, train_embeddings, k)\n",
    "    nearest_neighbors.append(nearest_embeds)\n",
    "\n",
    "mrr = mean_reciprocal_rank_2(nearest_neighbors, train_labels, test_labels) \n",
    "print(f\"MRR: {mrr}\")\n",
    "precision = precision_at_k_2(nearest_neighbors, train_labels, test_labels, k=100)\n",
    "hit_rate = hit_rate_at_k_2(nearest_neighbors, train_labels, test_labels, k=100)\n",
    "\n",
    "print(f\"Precision@100: {precision}\")\n",
    "print(f\"Hit Rate@100: {hit_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of comparisons per query: 50000.0\n"
     ]
    }
   ],
   "source": [
    "def average_comparisons(test_embeddings, train_embeddings):\n",
    "    total_comparisons = 0\n",
    "    for test_point in test_embeddings:\n",
    "        # For each test point, we compare it to every train sample (this is already done in find_k_nearest_neighbors)\n",
    "        total_comparisons += len(train_embeddings)\n",
    "        \n",
    "    avg_comparisons = total_comparisons / len(test_embeddings)\n",
    "    return avg_comparisons\n",
    "\n",
    "# Calculate the average number of comparisons for the queries\n",
    "avg_comparisons = average_comparisons(test_embeddings, train_embeddings)\n",
    "print(f\"Average number of comparisons per query: {avg_comparisons}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
