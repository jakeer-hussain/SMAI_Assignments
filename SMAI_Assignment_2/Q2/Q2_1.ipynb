{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative = False):\n",
    "    val = 1/(1 + np.exp(-x))\n",
    "    if(derivative):\n",
    "        return val * (1 - val)\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "def tanh(x, derivative = False):\n",
    "    val = np.tanh(x)\n",
    "    if derivative:\n",
    "        return 1 - val**2\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "def relu(x, derivative = False):\n",
    "    if derivative:\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def softmax(x):\n",
    "    max_x = np.max(x, axis=1, keepdims=True)\n",
    "    shifted_x = x - max_x\n",
    "    exp_x = np.exp(shifted_x)\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-9)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        print(f\"Warning: Unable to load image '{image_path}'\")\n",
    "        return None\n",
    "    return image.flatten() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sgd(model, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        indices = np.random.permutation(m)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i in indices:\n",
    "            X_batch = X[i:i+1]\n",
    "            y_batch = y[i:i+1]\n",
    "            total_loss += train_step(model, X_batch, y_batch, learning_rate)\n",
    "\n",
    "        return total_loss/m\n",
    "\n",
    "def train_minibatch(model, X, y, learning_rate, batch_size):\n",
    "        \"\"\" Mini-batch Gradient Descent: Updates weights after each mini-batch \"\"\"\n",
    "        m = X.shape[0]\n",
    "        indices = np.random.permutation(m)\n",
    "\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i in range(0, m, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            total_loss += train_step(model, X_batch, y_batch, learning_rate)  # Use returned loss\n",
    "            num_batches += 1\n",
    "\n",
    "        return total_loss / num_batches\n",
    "\n",
    "def train_batch(model, X, y, learning_rate):\n",
    "        m = X.shape[0]\n",
    "        indices = np.random.permutation(m)\n",
    "\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        return train_step(model, X_shuffled, y_shuffled, learning_rate)\n",
    "\n",
    "def train_step(model, X_batch, y_batch, learning_rate):\n",
    "        \"\"\" Performs forward, backward pass and updates weights \"\"\"\n",
    "        y_pred = model.forward(X_batch)\n",
    "        loss = cross_entropy_loss(y_pred, y_batch)\n",
    "        grads_w, grads_b = model.backward(X_batch, y_batch)\n",
    "\n",
    "        for i in range(len(model.weights)):\n",
    "            model.weights[i] -= learning_rate * grads_w[i]\n",
    "            model.biases[i] -= learning_rate * grads_b[i]\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layers, output_size, activation='relu', optimizer='batchgd', learning_rate=0.01, batch_size=None):\n",
    "        self.layers = [input_size] + hidden_layers + [output_size]\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            layer_input_size = self.layers[i]\n",
    "            layer_output_size = self.layers[i + 1]\n",
    "            \n",
    "            weight_matrix = np.random.randn(layer_input_size, layer_output_size) * 0.1\n",
    "            bias = np.zeros((1, layer_output_size))\n",
    "            \n",
    "            self.weights.append(weight_matrix)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "        activation_funs = {'sigmoid': sigmoid, 'tanh': tanh, 'relu': relu}\n",
    "        self.activation = activation_funs[activation]\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.a = []\n",
    "        self.a.append(X)\n",
    "        self.z_values = []\n",
    "\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            W = self.weights[i]\n",
    "            b = self.biases[i]\n",
    "            prev_a = self.a[-1]\n",
    "            \n",
    "            z = np.dot(prev_a, W) + b\n",
    "            self.z_values.append(z)\n",
    "\n",
    "            if i < len(self.weights) - 1:\n",
    "                activation_output = self.activation(z)\n",
    "            else:\n",
    "                activation_output = softmax(z)\n",
    "            \n",
    "            self.a.append(activation_output)\n",
    "\n",
    "        return self.a[-1]\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        grads_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grads_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        dz = self.a[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            W = self.weights[i]\n",
    "            a_prev = self.a[i]\n",
    "\n",
    "            dw = np.dot(a_prev.T, dz) / m\n",
    "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
    "\n",
    "            grads_w[i] = dw\n",
    "            grads_b[i] = db\n",
    "\n",
    "            if i > 0:\n",
    "                dz = np.dot(dz, W.T) * self.activation(self.z_values[i-1], derivative=True)\n",
    "\n",
    "        return grads_w, grads_b\n",
    "    \n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            if self.optimizer == 'sgd':\n",
    "                # print(\"sgd\")\n",
    "                loss_val = train_sgd(self, X, y, self.learning_rate)\n",
    "            elif self.optimizer == 'minibatchgd':\n",
    "                # print(\"mini\")\n",
    "                loss_val = train_minibatch(self, X, y, self.learning_rate, self.batch_size)\n",
    "            elif self.optimizer == 'batchgd':\n",
    "                # print(\"whole\")\n",
    "                loss_val = train_batch(self, X, y, self.learning_rate)\n",
    "\n",
    "            # print(f'Epoch {epoch}, Loss: {loss_val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_csv, symbol_to_index):\n",
    "    df_test = pd.read_csv(test_csv)\n",
    "    \n",
    "    X_test = np.array([flatten_image(path) for path in df_test[\"path\"] if flatten_image(path) is not None])\n",
    "    symbol_ids_test = df_test[\"symbol_id\"].values[:len(X_test)]\n",
    "    \n",
    "    # Convert symbol IDs to one-hot encoding\n",
    "    unique_symbols = list(symbol_to_index.keys())\n",
    "    y_true_one_hot = np.eye(len(unique_symbols))[np.array([symbol_to_index[s] for s in symbol_ids_test])]\n",
    "\n",
    "    y_pred_probs = model.forward(X_test)\n",
    "    y_pred_indices = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    index_to_symbol = {idx: symbol for symbol, idx in symbol_to_index.items()}\n",
    "    y_pred_symbols = np.array([index_to_symbol[idx] for idx in y_pred_indices])\n",
    "    \n",
    "    accuracy = np.mean(y_pred_symbols == symbol_ids_test) * 100\n",
    "    loss = cross_entropy_loss(y_pred_probs, y_true_one_hot)\n",
    "    del df_test, X_test\n",
    "    gc.collect()\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_executive(hidden_layers_list, activation, optimizer, learning_rate, batch_size):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        df = 0\n",
    "        df = pd.read_csv(f\"../fold-{i}/train.csv\")\n",
    "\n",
    "        X_train, symbol_ids = [], []\n",
    "\n",
    "        for path, symbol_id in zip(df[\"path\"], df[\"symbol_id\"]):\n",
    "            img = flatten_image(path)\n",
    "            if img is not None:\n",
    "                X_train.append(img)\n",
    "                symbol_ids.append(symbol_id)\n",
    "\n",
    "        X_train = np.array([img for img in X_train if img is not None])\n",
    "\n",
    "        symbol_ids = np.array(symbol_ids)\n",
    "\n",
    "        unique_symbols, indices = np.unique(symbol_ids, return_inverse=True)\n",
    "        symbol_to_index = {symbol: idx for idx, symbol in enumerate(unique_symbols)}\n",
    "        \n",
    "        # print(symbol_to_index)\n",
    "        y_train = np.eye(len(unique_symbols))[indices]\n",
    "        model = MLP(input_size=X_train.shape[1], hidden_layers = hidden_layers_list, output_size=len(unique_symbols), activation=activation, optimizer=optimizer, learning_rate=learning_rate, batch_size=batch_size)\n",
    "        \n",
    "        model.train(X_train, y_train, epochs=2)\n",
    "\n",
    "        train_accuracy, train_loss = test_model(model, f\"../fold-{i}/train.csv\", symbol_to_index)\n",
    "        test_accuracy, test_loss = test_model(model, f\"../fold-{i}/test.csv\", symbol_to_index)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"fold : {i}, train accuracy : {train_accuracy}, test accuracy : {test_accuracy}\")\n",
    "\n",
    "        del X_train, y_train, model  # Remove large variables\n",
    "        gc.collect()\n",
    "    \n",
    "    train_accuracy_mean = np.mean(train_accuracies)\n",
    "    train_accuracy_std = np.std(train_accuracies)\n",
    "    test_accuracy_mean = np.mean(test_accuracies)\n",
    "    test_accuracy_std = np.std(test_accuracies)\n",
    "    train_loss_mean = np.mean(train_losses)\n",
    "    train_loss_std = np.std(train_losses)\n",
    "    test_loss_mean = np.mean(test_losses)\n",
    "    test_loss_std = np.std(test_losses)\n",
    "\n",
    "    print(\"\\nFinal Statistics:\")\n",
    "    print(f\"Train Accuracy - Mean: {train_accuracy_mean:.2f}, Std: {train_accuracy_std:.2f}\")\n",
    "    print(f\"Test Accuracy - Mean: {test_accuracy_mean:.2f}, Std: {test_accuracy_std:.2f}\")\n",
    "    print(f\"Train loss - Mean: {train_loss_mean:.2f}, Std: {train_loss_std:.2f}\")\n",
    "    print(f\"Test loss - Mean: {test_loss_mean:.2f}, Std: {test_loss_std:.2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 1.8989559709338077, test accuracy : 1.9303201506591336\n",
      "fold : 2, train accuracy : 0.6338903283803077, test accuracy : 0.554735910298023\n",
      "fold : 3, train accuracy : 0.8386201427438541, test accuracy : 0.7213386152663631\n",
      "fold : 4, train accuracy : 0.8826755110400507, test accuracy : 0.7703703703703704\n",
      "fold : 5, train accuracy : 1.1889428316655106, test accuracy : 1.152155837985509\n",
      "fold : 6, train accuracy : 0.8208792587651811, test accuracy : 0.7733491969066032\n",
      "fold : 7, train accuracy : 0.6595626683568372, test accuracy : 0.5784483272705587\n",
      "fold : 8, train accuracy : 0.5993359779803433, test accuracy : 0.6514463303848912\n",
      "fold : 9, train accuracy : 0.46391922658131785, test accuracy : 0.5809078931608576\n",
      "fold : 10, train accuracy : 0.6966985986857731, test accuracy : 0.6242122321589341\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 0.87, Std: 0.39\n",
      "Test Accuracy - Mean: 0.83, Std: 0.40\n",
      "Train loss - Mean: 6.03, Std: 0.04\n",
      "Test loss - Mean: 6.03, Std: 0.04\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'relu', 'batchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 0.2261291580986637, test accuracy : 0.24717514124293788\n",
      "fold : 2, train accuracy : 0.7204801438316324, test accuracy : 0.719976394216583\n",
      "fold : 3, train accuracy : 0.07864128998149617, test accuracy : 0.08277656240761544\n",
      "fold : 4, train accuracy : 0.044265912604553447, test accuracy : 0.035555555555555556\n",
      "fold : 5, train accuracy : 1.1935664982330987, test accuracy : 1.193728471314883\n",
      "fold : 6, train accuracy : 0.12283470806944782, test accuracy : 0.12492563950029743\n",
      "fold : 7, train accuracy : 0.332752337189035, test accuracy : 0.3100960104955573\n",
      "fold : 8, train accuracy : 0.2620444749539607, test accuracy : 0.2629691608893139\n",
      "fold : 9, train accuracy : 1.9229880885603987, test accuracy : 1.8924422086477422\n",
      "fold : 10, train accuracy : 0.03562663288734067, test accuracy : 0.024008162775343615\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 0.49, Std: 0.59\n",
      "Test Accuracy - Mean: 0.49, Std: 0.58\n",
      "Train loss - Mean: 6.05, Std: 0.02\n",
      "Test loss - Mean: 6.05, Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'sigmoid', 'batchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 0.40399098128153077, test accuracy : 0.3766478342749529\n",
      "fold : 2, train accuracy : 0.46269367035058956, test accuracy : 0.4367069932133373\n",
      "fold : 3, train accuracy : 0.19825535289452814, test accuracy : 0.18329095961686276\n",
      "fold : 4, train accuracy : 0.46710448076745203, test accuracy : 0.4977777777777778\n",
      "fold : 5, train accuracy : 0.2661910895340005, test accuracy : 0.3385200142534743\n",
      "fold : 6, train accuracy : 0.46558316768258456, test accuracy : 0.41641879833432477\n",
      "fold : 7, train accuracy : 0.5479849997359109, test accuracy : 0.4234003220227801\n",
      "fold : 8, train accuracy : 1.2290347918495588, test accuracy : 1.3267989481233564\n",
      "fold : 9, train accuracy : 0.5477282475995645, test accuracy : 0.5329979638280034\n",
      "fold : 10, train accuracy : 0.7481592906341541, test accuracy : 0.684232639097293\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 0.53, Std: 0.27\n",
      "Test Accuracy - Mean: 0.52, Std: 0.30\n",
      "Train loss - Mean: 6.06, Std: 0.03\n",
      "Test loss - Mean: 6.06, Std: 0.03\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'tanh', 'batchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 71.0528229778962, test accuracy : 68.18502824858757\n",
      "fold : 2, train accuracy : 71.68777431124742, test accuracy : 68.50988492180583\n",
      "fold : 3, train accuracy : 72.73195876288659, test accuracy : 69.65056465440786\n",
      "fold : 4, train accuracy : 72.69718151666909, test accuracy : 69.14962962962963\n",
      "fold : 5, train accuracy : 72.51230225568877, test accuracy : 68.7136239458368\n",
      "fold : 6, train accuracy : 72.96909980650231, test accuracy : 69.60142772159429\n",
      "fold : 7, train accuracy : 72.76250462156024, test accuracy : 68.90691156300316\n",
      "fold : 8, train accuracy : 70.47808265291978, test accuracy : 66.919674874492\n",
      "fold : 9, train accuracy : 73.1283201900551, test accuracy : 69.12205054497545\n",
      "fold : 10, train accuracy : 72.6321484179136, test accuracy : 68.72936798511495\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 72.27, Std: 0.84\n",
      "Test Accuracy - Mean: 68.75, Std: 0.75\n",
      "Train loss - Mean: 1.07, Std: 0.03\n",
      "Test loss - Mean: 1.24, Std: 0.03\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'relu', 'minibatchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 41.529082722277685, test accuracy : 41.36064030131827\n",
      "fold : 2, train accuracy : 42.07934535455555, test accuracy : 42.20714074948362\n",
      "fold : 3, train accuracy : 42.544937879989426, test accuracy : 42.03866847986756\n",
      "fold : 4, train accuracy : 41.99381598594062, test accuracy : 41.68888888888888\n",
      "fold : 5, train accuracy : 42.225304666600614, test accuracy : 41.72110701983608\n",
      "fold : 6, train accuracy : 42.55694313281338, test accuracy : 41.77275431290898\n",
      "fold : 7, train accuracy : 42.3110442085248, test accuracy : 42.01800942214801\n",
      "fold : 8, train accuracy : 41.35550260394321, test accuracy : 41.638775998087496\n",
      "fold : 9, train accuracy : 42.63305506978586, test accuracy : 42.65181458857349\n",
      "fold : 10, train accuracy : 42.84564431425329, test accuracy : 42.5784766820719\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 42.21, Std: 0.46\n",
      "Test Accuracy - Mean: 41.97, Std: 0.39\n",
      "Train loss - Mean: 2.90, Std: 0.03\n",
      "Test loss - Mean: 2.92, Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'sigmoid', 'minibatchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 67.01423555781831, test accuracy : 64.58921845574388\n",
      "fold : 2, train accuracy : 66.9061657236529, test accuracy : 64.8686928297433\n",
      "fold : 3, train accuracy : 67.15305313243456, test accuracy : 65.27523207000532\n",
      "fold : 4, train accuracy : 67.39452159780124, test accuracy : 64.71703703703704\n",
      "fold : 5, train accuracy : 67.19838832193929, test accuracy : 64.27723007483074\n",
      "fold : 6, train accuracy : 67.58286389782265, test accuracy : 64.93753718024985\n",
      "fold : 7, train accuracy : 67.32160777478477, test accuracy : 64.60134772496869\n",
      "fold : 8, train accuracy : 66.72035168084699, test accuracy : 64.20631125986135\n",
      "fold : 9, train accuracy : 67.20493615336392, test accuracy : 64.77422445801892\n",
      "fold : 10, train accuracy : 67.35412873090016, test accuracy : 64.25184562751336\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 67.19, Std: 0.24\n",
      "Test Accuracy - Mean: 64.65, Std: 0.32\n",
      "Train loss - Mean: 1.38, Std: 0.01\n",
      "Test loss - Mean: 1.49, Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'tanh', 'minibatchgd', 0.01, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 66.21352675531107, test accuracy : 64.34792843691149\n",
      "fold : 2, train accuracy : 66.9696208555867, test accuracy : 65.20507524343464\n",
      "fold : 3, train accuracy : 68.03991541104944, test accuracy : 65.97883285047004\n",
      "fold : 4, train accuracy : 66.91023930020216, test accuracy : 64.75851851851851\n",
      "fold : 5, train accuracy : 67.10261237161069, test accuracy : 64.22377954626441\n",
      "fold : 6, train accuracy : 66.56848695376529, test accuracy : 64.07495538370019\n",
      "fold : 7, train accuracy : 65.89222521523266, test accuracy : 63.05683105730813\n",
      "fold : 8, train accuracy : 67.3045062408829, test accuracy : 65.00717188620607\n",
      "fold : 9, train accuracy : 67.51047612762729, test accuracy : 65.12756018684873\n",
      "fold : 10, train accuracy : 67.07505343994933, test accuracy : 64.90006602244763\n",
      "\n",
      "Final Statistics:\n",
      "Train Accuracy - Mean: 66.96, Std: 0.59\n",
      "Test Accuracy - Mean: 64.67, Std: 0.75\n",
      "Train loss - Mean: 1.27, Std: 0.02\n",
      "Test loss - Mean: 1.40, Std: 0.04\n"
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'relu', 'sgd', 0.001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 29.581264339696244, test accuracy : 29.384416195856872\n",
      "fold : 2, train accuracy : 30.2925545978531, test accuracy : 31.03570374741812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmlp_executive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msgd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 32\u001b[0m, in \u001b[0;36mmlp_executive\u001b[0;34m(hidden_layers_list, activation, optimizer, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mlen\u001b[39m(unique_symbols))[indices]\n\u001b[1;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_layers \u001b[38;5;241m=\u001b[39m hidden_layers_list, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(unique_symbols), activation\u001b[38;5;241m=\u001b[39mactivation, optimizer\u001b[38;5;241m=\u001b[39moptimizer, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m train_accuracy, train_loss \u001b[38;5;241m=\u001b[39m test_model(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../fold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, symbol_to_index)\n\u001b[1;32m     35\u001b[0m test_accuracy, test_loss \u001b[38;5;241m=\u001b[39m test_model(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../fold-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, symbol_to_index)\n",
      "Cell \u001b[0;32mIn[6], line 74\u001b[0m, in \u001b[0;36mMLP.train\u001b[0;34m(self, X, y, epochs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msgd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;66;03m# print(\"sgd\")\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m         loss_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminibatchgd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m# print(\"mini\")\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         loss_val \u001b[38;5;241m=\u001b[39m train_minibatch(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mtrain_sgd\u001b[0;34m(model, X, y, learning_rate)\u001b[0m\n\u001b[1;32m      7\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m X[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      8\u001b[0m     y_batch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\u001b[38;5;241m/\u001b[39mm\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, X_batch, y_batch, learning_rate)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model, X_batch, y_batch, learning_rate):\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124;03m\"\"\" Performs forward, backward pass and updates weights \"\"\"\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m         loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(y_pred, y_batch)\n\u001b[1;32m     44\u001b[0m         grads_w, grads_b \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbackward(X_batch, y_batch)\n",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     32\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases[i]\n\u001b[1;32m     33\u001b[0m prev_a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 35\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m b\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_values\u001b[38;5;241m.\u001b[39mappend(z)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'sigmoid', 'sgd', 0.001, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold : 1, train accuracy : 60.74873876792669, test accuracy : 59.53389830508474\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "mlp_executive([256, 128], 'tanh', 'sgd', 0.001, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and Standard Deviation of Accuracy:\n",
    "\n",
    "Mean accuracy: This represents the average accuracy across all 10 folds. It gives you a general idea of how well the model performs across different subsets of data. A higher mean accuracy suggests that, on average, your model performs well across all folds.\n",
    "\n",
    "Standard deviation: This shows how much the model's accuracy varies between the different folds. If the standard deviation is low, the model’s performance is consistent across different train-test splits. If the standard deviation is high, the model's performance is fluctuating significantly from fold to fold, indicating that the model might not generalize well across different data distributions.\n",
    "Impact of High vs. Low Standard Deviation:\n",
    "\n",
    "High standard deviation: A high standard deviation in accuracy across the 10 folds suggests that the model’s performance is inconsistent. This could indicate that the model is highly sensitive to specific data splits, and might not generalize well to new, unseen data. For example, if the model performs very well on some folds but poorly on others, its generalization ability may be questionable.\n",
    "Low standard deviation: A low standard deviation in accuracy suggests that the model is stable, meaning its performance is consistent across different data splits. This is a good sign of the model's generalization ability, as it implies that the model is not overfitting to specific parts of the data.\n",
    "Choosing Between Two Configurations:\n",
    "\n",
    "When comparing two configurations where one has a slightly higher mean accuracy but a significantly higher standard deviation compared to another with marginally lower mean accuracy and lower standard deviation, it’s better to choose the configuration with lower standard deviation, even if the mean accuracy is slightly lower.\n",
    "This is because model consistency is important for generalization. A configuration with high variance (high standard deviation) suggests that the model's performance might be sensitive to the specific data split, which could lead to poor real-world performance. A model with a lower standard deviation implies that it is stable and less likely to be affected by specific data variations, which is desirable for robustness and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
